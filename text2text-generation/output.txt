[2024-05-11 07:28:38,721] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-11 07:28:40,627] [INFO] [runner.py:452:main] Using IP address of 10.0.16.14 for node 10.0.16.14
[2024-05-11 07:28:40,629] [INFO] [multinode_runner.py:70:get_cmd] Running on the following workers: 10.0.16.14,10.0.16.13
[2024-05-11 07:28:40,629] [INFO] [runner.py:555:main] cmd = pdsh -S -f 1024 -w 10.0.16.14,10.0.16.13 export PYTHONPATH=/home/kechengsheji/DeepSpeedExamples/inference/huggingface/text2text-generation;  cd /home/kechengsheji/DeepSpeedExamples/inference/huggingface/text2text-generation; /home/kechengsheji/anaconda3/envs/Test04/bin/python -u -m deepspeed.launcher.launch --world_info=eyIxMC4wLjE2LjE0IjogWzAsIDFdLCAiMTAuMC4xNi4xMyI6IFswLCAxXX0= --node_rank=%n --master_addr=10.0.16.14 --master_port=29500 test-t5.py --batch_size '256'
10.0.16.13: [2024-05-11 15:28:42,315] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
10.0.16.14: [2024-05-11 07:28:42,591] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
10.0.16.13: [2024-05-11 15:28:42,724] [INFO] [launch.py:145:main] WORLD INFO DICT: {'10.0.16.14': [0, 1], '10.0.16.13': [0, 1]}
10.0.16.13: [2024-05-11 15:28:42,724] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=2, node_rank=1
10.0.16.13: [2024-05-11 15:28:42,724] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'10.0.16.14': [0, 1], '10.0.16.13': [2, 3]})
10.0.16.13: [2024-05-11 15:28:42,724] [INFO] [launch.py:163:main] dist_world_size=4
10.0.16.13: [2024-05-11 15:28:42,724] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
10.0.16.14: [2024-05-11 07:28:42,906] [INFO] [launch.py:145:main] WORLD INFO DICT: {'10.0.16.14': [0, 1], '10.0.16.13': [0, 1]}
10.0.16.14: [2024-05-11 07:28:42,906] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=2, node_rank=0
10.0.16.14: [2024-05-11 07:28:42,906] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'10.0.16.14': [0, 1], '10.0.16.13': [2, 3]})
10.0.16.14: [2024-05-11 07:28:42,906] [INFO] [launch.py:163:main] dist_world_size=4
10.0.16.14: [2024-05-11 07:28:42,906] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
10.0.16.14: [2024-05-11 07:28:44,558] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
10.0.16.14: [2024-05-11 07:28:44,622] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
10.0.16.13: [2024-05-11 15:28:44,665] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
10.0.16.13: [2024-05-11 15:28:44,743] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
10.0.16.14: [2024-05-11 07:28:48,060] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5+fc9e1ee0, git-hash=fc9e1ee0, git-branch=HEAD
10.0.16.14: config_dict: {'mp_size': 4, 'dtype': torch.float32, 'tp_proportion': (4, 2)}
10.0.16.14: [2024-05-11 07:28:48,060] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead
10.0.16.14: ds_inference_config: replace_with_kernel_inject=False dtype=torch.float32 tensor_parallel=DeepSpeedTPConfig(enabled=True, tp_size=4, mpu=None, tp_group=None) enable_cuda_graph=False zero=DeepSpeedZeroConfig(stage=0, contiguous_gradients=True, reduce_scatter=True, reduce_bucket_size=500,000,000, allgather_partitions=True, allgather_bucket_size=500,000,000, overlap_comm=False, load_from_fp32_weights=True, elastic_checkpoint=False, offload_param=None, offload_optimizer=None, sub_group_size=1,000,000,000, cpu_offload_param=None, cpu_offload_use_pin_memory=None, cpu_offload=None, prefetch_bucket_size=50,000,000, param_persistence_threshold=100,000, model_persistence_threshold=sys.maxsize, max_live_parameters=1,000,000,000, max_reuse_distance=1,000,000,000, gather_16bit_weights_on_model_save=False, stage3_gather_fp16_weights_on_model_save=False, ignore_unused_parameters=True, legacy_stage1=False, round_robin_gradients=False, mics_shard_size=-1, mics_hierarchical_params_gather=False, memory_efficient_linear=True) triangular_masking=True moe=DeepSpeedMoEConfig(enabled=True, ep_size=1, moe_experts=[1], type='standard', ep_mp_group=None, ep_group=None) quant=QuantizationConfig(enabled=True, activation=ActivationQuantConfig(q_type='symmetric', q_groups=1, enabled=True, num_bits=8), weight=WeightQuantConfig(q_type='symmetric', q_groups=1, enabled=True, num_bits=8), qkv=QKVQuantConfig(enabled=True)) checkpoint=None base_dir=None set_empty_params=False save_mp_checkpoint_path=None checkpoint_config=InferenceCheckpointConfig(checkpoint_dir=None, save_mp_checkpoint_path=None, base_dir=None) return_tuple=True training_mp_size=1 replace_method='auto' injection_policy=None injection_policy_tuple=None config=None max_out_tokens=1024 min_out_tokens=1 transposed_mode=False mpu=None ep_size=1 ep_group=None ep_mp_group=None moe_experts=[1] moe_type='standard' tp_proportion=(4, 2)
10.0.16.14: [2024-05-11 07:28:48,061] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1
10.0.16.14: [2024-05-11 07:28:48,063] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
10.0.16.14: [2024-05-11 07:28:48,063] [INFO] [comm.py:594:init_distributed] cdb=None
10.0.16.14: [2024-05-11 07:28:48,063] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
10.0.16.14: [2024-05-11 07:28:48,068] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5+fc9e1ee0, git-hash=fc9e1ee0, git-branch=HEAD
10.0.16.14: config_dict: {'mp_size': 4, 'dtype': torch.float32, 'tp_proportion': (4, 2)}
10.0.16.14: [2024-05-11 07:28:48,069] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead
10.0.16.14: ds_inference_config: replace_with_kernel_inject=False dtype=torch.float32 tensor_parallel=DeepSpeedTPConfig(enabled=True, tp_size=4, mpu=None, tp_group=None) enable_cuda_graph=False zero=DeepSpeedZeroConfig(stage=0, contiguous_gradients=True, reduce_scatter=True, reduce_bucket_size=500,000,000, allgather_partitions=True, allgather_bucket_size=500,000,000, overlap_comm=False, load_from_fp32_weights=True, elastic_checkpoint=False, offload_param=None, offload_optimizer=None, sub_group_size=1,000,000,000, cpu_offload_param=None, cpu_offload_use_pin_memory=None, cpu_offload=None, prefetch_bucket_size=50,000,000, param_persistence_threshold=100,000, model_persistence_threshold=sys.maxsize, max_live_parameters=1,000,000,000, max_reuse_distance=1,000,000,000, gather_16bit_weights_on_model_save=False, stage3_gather_fp16_weights_on_model_save=False, ignore_unused_parameters=True, legacy_stage1=False, round_robin_gradients=False, mics_shard_size=-1, mics_hierarchical_params_gather=False, memory_efficient_linear=True) triangular_masking=True moe=DeepSpeedMoEConfig(enabled=True, ep_size=1, moe_experts=[1], type='standard', ep_mp_group=None, ep_group=None) quant=QuantizationConfig(enabled=True, activation=ActivationQuantConfig(q_type='symmetric', q_groups=1, enabled=True, num_bits=8), weight=WeightQuantConfig(q_type='symmetric', q_groups=1, enabled=True, num_bits=8), qkv=QKVQuantConfig(enabled=True)) checkpoint=None base_dir=None set_empty_params=False save_mp_checkpoint_path=None checkpoint_config=InferenceCheckpointConfig(checkpoint_dir=None, save_mp_checkpoint_path=None, base_dir=None) return_tuple=True training_mp_size=1 replace_method='auto' injection_policy=None injection_policy_tuple=None config=None max_out_tokens=1024 min_out_tokens=1 transposed_mode=False mpu=None ep_size=1 ep_group=None ep_mp_group=None moe_experts=[1] moe_type='standard' tp_proportion=(4, 2)
10.0.16.14: [2024-05-11 07:28:48,069] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1
10.0.16.14: [2024-05-11 07:28:48,072] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
10.0.16.14: [2024-05-11 07:28:48,072] [INFO] [comm.py:594:init_distributed] cdb=None
10.0.16.13: [2024-05-11 15:28:48,437] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5+fc9e1ee0, git-hash=fc9e1ee0, git-branch=HEAD
10.0.16.13: config_dict: {'mp_size': 4, 'dtype': torch.float32, 'tp_proportion': (4, 2)}
10.0.16.13: [2024-05-11 15:28:48,438] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead
10.0.16.13: ds_inference_config: replace_with_kernel_inject=False dtype=torch.float32 tensor_parallel=DeepSpeedTPConfig(enabled=True, tp_size=4, mpu=None, tp_group=None) enable_cuda_graph=False zero=DeepSpeedZeroConfig(stage=0, contiguous_gradients=True, reduce_scatter=True, reduce_bucket_size=500,000,000, allgather_partitions=True, allgather_bucket_size=500,000,000, overlap_comm=False, load_from_fp32_weights=True, elastic_checkpoint=False, offload_param=None, offload_optimizer=None, sub_group_size=1,000,000,000, cpu_offload_param=None, cpu_offload_use_pin_memory=None, cpu_offload=None, prefetch_bucket_size=50,000,000, param_persistence_threshold=100,000, model_persistence_threshold=sys.maxsize, max_live_parameters=1,000,000,000, max_reuse_distance=1,000,000,000, gather_16bit_weights_on_model_save=False, stage3_gather_fp16_weights_on_model_save=False, ignore_unused_parameters=True, legacy_stage1=False, round_robin_gradients=False, mics_shard_size=-1, mics_hierarchical_params_gather=False, memory_efficient_linear=True) triangular_masking=True moe=DeepSpeedMoEConfig(enabled=True, ep_size=1, moe_experts=[1], type='standard', ep_mp_group=None, ep_group=None) quant=QuantizationConfig(enabled=True, activation=ActivationQuantConfig(q_type='symmetric', q_groups=1, enabled=True, num_bits=8), weight=WeightQuantConfig(q_type='symmetric', q_groups=1, enabled=True, num_bits=8), qkv=QKVQuantConfig(enabled=True)) checkpoint=None base_dir=None set_empty_params=False save_mp_checkpoint_path=None checkpoint_config=InferenceCheckpointConfig(checkpoint_dir=None, save_mp_checkpoint_path=None, base_dir=None) return_tuple=True training_mp_size=1 replace_method='auto' injection_policy=None injection_policy_tuple=None config=None max_out_tokens=1024 min_out_tokens=1 transposed_mode=False mpu=None ep_size=1 ep_group=None ep_mp_group=None moe_experts=[1] moe_type='standard' tp_proportion=(4, 2)
10.0.16.13: [2024-05-11 15:28:48,438] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1
10.0.16.13: [2024-05-11 15:28:48,440] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
10.0.16.13: [2024-05-11 15:28:48,440] [INFO] [comm.py:594:init_distributed] cdb=None
10.0.16.13: [2024-05-11 15:28:48,455] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5+fc9e1ee0, git-hash=fc9e1ee0, git-branch=HEAD
10.0.16.13: config_dict: {'mp_size': 4, 'dtype': torch.float32, 'tp_proportion': (4, 2)}
10.0.16.13: [2024-05-11 15:28:48,456] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead
10.0.16.13: ds_inference_config: replace_with_kernel_inject=False dtype=torch.float32 tensor_parallel=DeepSpeedTPConfig(enabled=True, tp_size=4, mpu=None, tp_group=None) enable_cuda_graph=False zero=DeepSpeedZeroConfig(stage=0, contiguous_gradients=True, reduce_scatter=True, reduce_bucket_size=500,000,000, allgather_partitions=True, allgather_bucket_size=500,000,000, overlap_comm=False, load_from_fp32_weights=True, elastic_checkpoint=False, offload_param=None, offload_optimizer=None, sub_group_size=1,000,000,000, cpu_offload_param=None, cpu_offload_use_pin_memory=None, cpu_offload=None, prefetch_bucket_size=50,000,000, param_persistence_threshold=100,000, model_persistence_threshold=sys.maxsize, max_live_parameters=1,000,000,000, max_reuse_distance=1,000,000,000, gather_16bit_weights_on_model_save=False, stage3_gather_fp16_weights_on_model_save=False, ignore_unused_parameters=True, legacy_stage1=False, round_robin_gradients=False, mics_shard_size=-1, mics_hierarchical_params_gather=False, memory_efficient_linear=True) triangular_masking=True moe=DeepSpeedMoEConfig(enabled=True, ep_size=1, moe_experts=[1], type='standard', ep_mp_group=None, ep_group=None) quant=QuantizationConfig(enabled=True, activation=ActivationQuantConfig(q_type='symmetric', q_groups=1, enabled=True, num_bits=8), weight=WeightQuantConfig(q_type='symmetric', q_groups=1, enabled=True, num_bits=8), qkv=QKVQuantConfig(enabled=True)) checkpoint=None base_dir=None set_empty_params=False save_mp_checkpoint_path=None checkpoint_config=InferenceCheckpointConfig(checkpoint_dir=None, save_mp_checkpoint_path=None, base_dir=None) return_tuple=True training_mp_size=1 replace_method='auto' injection_policy=None injection_policy_tuple=None config=None max_out_tokens=1024 min_out_tokens=1 transposed_mode=False mpu=None ep_size=1 ep_group=None ep_mp_group=None moe_experts=[1] moe_type='standard' tp_proportion=(4, 2)
10.0.16.13: [2024-05-11 15:28:48,456] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1
10.0.16.13: [2024-05-11 15:28:48,458] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
10.0.16.13: [2024-05-11 15:28:48,458] [INFO] [comm.py:594:init_distributed] cdb=None
10.0.16.13: tensor_parallel_group: <torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0de4e85ff0>
10.0.16.14: tensor_parallel_group: <torch.distributed.distributed_c10d.ProcessGroup object at 0x7f53d4aa8630>
10.0.16.13: !deepspeed engine has no moe layers
10.0.16.13: tensor_parallel_group: <torch.distributed.distributed_c10d.ProcessGroup object at 0x7ffb16fcb230>
10.0.16.14: tensor_parallel_group: <torch.distributed.distributed_c10d.ProcessGroup object at 0x7f967d5e8b70>
10.0.16.13: !deepspeed engine has no moe layers
10.0.16.14: _replace_module: T5ForConditionalGeneration(
10.0.16.14:   (shared): Embedding(32128, 768)
10.0.16.14:   (encoder): T5Stack(
10.0.16.14:     (embed_tokens): Embedding(32128, 768)
10.0.16.14:     (block): ModuleList(
10.0.16.14:       (0): T5Block(
10.0.16.14:         (layer): ModuleList(
10.0.16.14:           (0): T5LayerSelfAttention(
10.0.16.14:             (SelfAttention): T5Attention(
10.0.16.14:               (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (relative_attention_bias): Embedding(32, 12)
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:           (1): T5LayerFF(
10.0.16.14:             (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:               (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:               (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:               (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:               (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:               (act): NewGELUActivation()
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:         )
10.0.16.14:       )
10.0.16.14:       (1-11): 11 x T5Block(
10.0.16.14:         (layer): ModuleList(
10.0.16.14:           (0): T5LayerSelfAttention(
10.0.16.14:             (SelfAttention): T5Attention(
10.0.16.14:               (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:           (1): T5LayerFF(
10.0.16.14:             (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:               (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:               (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:               (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:               (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:               (act): NewGELUActivation()
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:         )
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:     (final_layer_norm): T5LayerNorm()
10.0.16.14:     (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:   )
10.0.16.14:   (decoder): T5Stack(
10.0.16.14:     (embed_tokens): Embedding(32128, 768)
10.0.16.14:     (block): ModuleList(
10.0.16.14:       (0): T5Block(
10.0.16.14:         (layer): ModuleList(
10.0.16.14:           (0): T5LayerSelfAttention(
10.0.16.14:             (SelfAttention): T5Attention(
10.0.16.14:               (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (relative_attention_bias): Embedding(32, 12)
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:           (1): T5LayerCrossAttention(
10.0.16.14:             (EncDecAttention): T5Attention(
10.0.16.14:               (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:           (2): T5LayerFF(
10.0.16.14:             (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:               (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:               (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:               (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:               (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:               (act): NewGELUActivation()
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:         )
10.0.16.14:       )
10.0.16.14:       (1-11): 11 x T5Block(
10.0.16.14:         (layer): ModuleList(
10.0.16.14:           (0): T5LayerSelfAttention(
10.0.16.14:             (SelfAttention): T5Attention(
10.0.16.14:               (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:           (1): T5LayerCrossAttention(
10.0.16.14:             (EncDecAttention): T5Attention(
10.0.16.14:               (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:           (2): T5LayerFF(
10.0.16.14:             (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:               (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:               (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:               (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:               (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:               (act): NewGELUActivation()
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:         )
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:     (final_layer_norm): T5LayerNorm()
10.0.16.14:     (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:   )
10.0.16.14:   (lm_head): Linear(in_features=768, out_features=32128, bias=False)
10.0.16.14: )
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: _replace_module: Embedding(32128, 768)
10.0.16.13: slice embedding
10.0.16.13: slice embedding
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: _replace_module: T5Stack(
10.0.16.14:   (embed_tokens): Embedding(32128, 768)
10.0.16.14:   (block): ModuleList(
10.0.16.14:     (0): T5Block(
10.0.16.14:       (layer): ModuleList(
10.0.16.14:         (0): T5LayerSelfAttention(
10.0.16.14:           (SelfAttention): T5Attention(
10.0.16.14:             (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (relative_attention_bias): Embedding(32, 12)
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:         (1): T5LayerFF(
10.0.16.14:           (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:             (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:             (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:             (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:             (act): NewGELUActivation()
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:     (1-11): 11 x T5Block(
10.0.16.14:       (layer): ModuleList(
10.0.16.14:         (0): T5LayerSelfAttention(
10.0.16.14:           (SelfAttention): T5Attention(
10.0.16.14:             (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:         (1): T5LayerFF(
10.0.16.14:           (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:             (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:             (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:             (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:             (act): NewGELUActivation()
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:   )
10.0.16.14:   (final_layer_norm): T5LayerNorm()
10.0.16.14:   (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14: )
10.0.16.14: _replace_module: Embedding(32128, 768)
10.0.16.14: _replace_module: ModuleList(
10.0.16.14:   (0): T5Block(
10.0.16.14:     (layer): ModuleList(
10.0.16.14:       (0): T5LayerSelfAttention(
10.0.16.14:         (SelfAttention): T5Attention(
10.0.16.14:           (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (relative_attention_bias): Embedding(32, 12)
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:       (1): T5LayerFF(
10.0.16.14:         (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:           (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:           (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:           (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           (act): NewGELUActivation()
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:   )
10.0.16.14:   (1-11): 11 x T5Block(
10.0.16.14:     (layer): ModuleList(
10.0.16.14:       (0): T5LayerSelfAttention(
10.0.16.14:         (SelfAttention): T5Attention(
10.0.16.14:           (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14:       (1): T5LayerFF(
10.0.16.14:         (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:           (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:           (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:           (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           (act): NewGELUActivation()
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.13: replace without policy
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:   )
10.0.16.14: )
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: slice embedding
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.13: replace without policy
10.0.16.14: n_heads: 12
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: slice embedding
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.13: replace without policy
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.13: slice embedding
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: slice embedding
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: _replace_module: T5LayerNorm()
10.0.16.14: _replace_module: Dropout(p=0.1, inplace=False)
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: _replace_module: T5Stack(
10.0.16.14:   (embed_tokens): Embedding(32128, 768)
10.0.16.14:   (block): ModuleList(
10.0.16.14:     (0): T5Block(
10.0.16.14:       (layer): ModuleList(
10.0.16.14:         (0): T5LayerSelfAttention(
10.0.16.14:           (SelfAttention): T5Attention(
10.0.16.14:             (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (relative_attention_bias): Embedding(32, 12)
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:         (1): T5LayerCrossAttention(
10.0.16.14:           (EncDecAttention): T5Attention(
10.0.16.14:             (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:         (2): T5LayerFF(
10.0.16.14:           (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:             (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:             (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:             (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:             (act): NewGELUActivation()
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:     (1-11): 11 x T5Block(
10.0.16.14:       (layer): ModuleList(
10.0.16.14:         (0): T5LayerSelfAttention(
10.0.16.14:           (SelfAttention): T5Attention(
10.0.16.14:             (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.13: replace without policy
10.0.16.14:             (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14:         (1): T5LayerCrossAttention(
10.0.16.14:           (EncDecAttention): T5Attention(
10.0.16.14:             (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:         (2): T5LayerFF(
10.0.16.14:           (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:             (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:             (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:             (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:             (act): NewGELUActivation()
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:   )
10.0.16.14:   (final_layer_norm): T5LayerNorm()
10.0.16.14:   (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14: )
10.0.16.14: _replace_module: Embedding(32128, 768)
10.0.16.14: _replace_module: ModuleList(
10.0.16.14:   (0): T5Block(
10.0.16.14:     (layer): ModuleList(
10.0.16.14:       (0): T5LayerSelfAttention(
10.0.16.14:         (SelfAttention): T5Attention(
10.0.16.14:           (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (relative_attention_bias): Embedding(32, 12)
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:       (1): T5LayerCrossAttention(
10.0.16.14:         (EncDecAttention): T5Attention(
10.0.16.14:           (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:       (2): T5LayerFF(
10.0.16.14:         (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:           (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:           (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:           (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           (act): NewGELUActivation()
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:   )
10.0.16.14:   (1-11): 11 x T5Block(
10.0.16.14:     (layer): ModuleList(
10.0.16.14:       (0): T5LayerSelfAttention(
10.0.16.14:         (SelfAttention): T5Attention(
10.0.16.14:           (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:       (1): T5LayerCrossAttention(
10.0.16.14:         (EncDecAttention): T5Attention(
10.0.16.14:           (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:       (2): T5LayerFF(
10.0.16.14:         (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:           (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:           (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:           (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           (act): NewGELUActivation()
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:   )
10.0.16.14: )
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: slice embedding
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: slice embedding
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('SelfAttention.o', 'DenseReluDense.wo', 'EncDecAttention.o')
10.0.16.14: _replace_module: T5LayerNorm()
10.0.16.14: _replace_module: Dropout(p=0.1, inplace=False)
10.0.16.14: _replace_module: Linear(in_features=768, out_features=32128, bias=False)
10.0.16.14: [2024-05-11 07:28:54,919] [INFO] [launch.py:347:main] Process 26432 exits successfully.
10.0.16.14: rank: 0 end2end time is 303.3343685997857 ms
10.0.16.14: rank: 0 model time is 254.50472980075412 ms
10.0.16.13: rank: 2 end2end time is 323.3809471130371 ms
10.0.16.13: rank: 2 model time is 260.68532763587103 ms
10.0.16.14: [2024-05-11 07:28:59,925] [INFO] [launch.py:347:main] Process 26431 exits successfully.
10.0.16.13: [2024-05-11 15:29:00,745] [INFO] [launch.py:347:main] Process 2169390 exits successfully.
10.0.16.13: [2024-05-11 15:29:00,746] [INFO] [launch.py:347:main] Process 2169389 exits successfully.
