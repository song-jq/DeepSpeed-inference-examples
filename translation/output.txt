[2024-05-11 08:00:30,261] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-11 08:00:32,172] [INFO] [runner.py:452:main] Using IP address of 10.0.16.14 for node 10.0.16.14
[2024-05-11 08:00:32,173] [INFO] [multinode_runner.py:70:get_cmd] Running on the following workers: 10.0.16.14,10.0.16.13
[2024-05-11 08:00:32,174] [INFO] [runner.py:555:main] cmd = pdsh -S -f 1024 -w 10.0.16.14,10.0.16.13 export PYTHONPATH=/home/kechengsheji/DeepSpeedExamples/inference/huggingface/translation;  cd /home/kechengsheji/DeepSpeedExamples/inference/huggingface/translation; /home/kechengsheji/anaconda3/envs/Test04/bin/python -u -m deepspeed.launcher.launch --world_info=eyIxMC4wLjE2LjE0IjogWzAsIDFdLCAiMTAuMC4xNi4xMyI6IFswLCAxXX0= --node_rank=%n --master_addr=10.0.16.14 --master_port=29500 test-t5.py --batch_size '16'
10.0.16.13: [2024-05-11 16:00:33,858] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
10.0.16.14: [2024-05-11 08:00:34,144] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
10.0.16.13: [2024-05-11 16:00:34,270] [INFO] [launch.py:145:main] WORLD INFO DICT: {'10.0.16.14': [0, 1], '10.0.16.13': [0, 1]}
10.0.16.13: [2024-05-11 16:00:34,270] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=2, node_rank=1
10.0.16.13: [2024-05-11 16:00:34,270] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'10.0.16.14': [0, 1], '10.0.16.13': [2, 3]})
10.0.16.13: [2024-05-11 16:00:34,270] [INFO] [launch.py:163:main] dist_world_size=4
10.0.16.13: [2024-05-11 16:00:34,270] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
10.0.16.14: [2024-05-11 08:00:34,460] [INFO] [launch.py:145:main] WORLD INFO DICT: {'10.0.16.14': [0, 1], '10.0.16.13': [0, 1]}
10.0.16.14: [2024-05-11 08:00:34,460] [INFO] [launch.py:151:main] nnodes=2, num_local_procs=2, node_rank=0
10.0.16.14: [2024-05-11 08:00:34,460] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'10.0.16.14': [0, 1], '10.0.16.13': [2, 3]})
10.0.16.14: [2024-05-11 08:00:34,460] [INFO] [launch.py:163:main] dist_world_size=4
10.0.16.14: [2024-05-11 08:00:34,461] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
10.0.16.14: [2024-05-11 08:00:36,141] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
10.0.16.14: [2024-05-11 08:00:36,193] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
10.0.16.13: [2024-05-11 16:00:36,225] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
10.0.16.13: [2024-05-11 16:00:36,228] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
10.0.16.14: [2024-05-11 08:00:39,760] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5+fc9e1ee0, git-hash=fc9e1ee0, git-branch=HEAD
10.0.16.14: config_dict: {'mp_size': 4, 'dtype': torch.float32, 'tp_proportion': (3, 3)}
10.0.16.14: [2024-05-11 08:00:39,761] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead
10.0.16.14: ds_inference_config: replace_with_kernel_inject=False dtype=torch.float32 tensor_parallel=DeepSpeedTPConfig(enabled=True, tp_size=4, mpu=None, tp_group=None) enable_cuda_graph=False zero=DeepSpeedZeroConfig(stage=0, contiguous_gradients=True, reduce_scatter=True, reduce_bucket_size=500,000,000, allgather_partitions=True, allgather_bucket_size=500,000,000, overlap_comm=False, load_from_fp32_weights=True, elastic_checkpoint=False, offload_param=None, offload_optimizer=None, sub_group_size=1,000,000,000, cpu_offload_param=None, cpu_offload_use_pin_memory=None, cpu_offload=None, prefetch_bucket_size=50,000,000, param_persistence_threshold=100,000, model_persistence_threshold=sys.maxsize, max_live_parameters=1,000,000,000, max_reuse_distance=1,000,000,000, gather_16bit_weights_on_model_save=False, stage3_gather_fp16_weights_on_model_save=False, ignore_unused_parameters=True, legacy_stage1=False, round_robin_gradients=False, mics_shard_size=-1, mics_hierarchical_params_gather=False, memory_efficient_linear=True) triangular_masking=True moe=DeepSpeedMoEConfig(enabled=True, ep_size=1, moe_experts=[1], type='standard', ep_mp_group=None, ep_group=None) quant=QuantizationConfig(enabled=True, activation=ActivationQuantConfig(q_type='symmetric', q_groups=1, enabled=True, num_bits=8), weight=WeightQuantConfig(q_type='symmetric', q_groups=1, enabled=True, num_bits=8), qkv=QKVQuantConfig(enabled=True)) checkpoint=None base_dir=None set_empty_params=False save_mp_checkpoint_path=None checkpoint_config=InferenceCheckpointConfig(checkpoint_dir=None, save_mp_checkpoint_path=None, base_dir=None) return_tuple=True training_mp_size=1 replace_method='auto' injection_policy=None injection_policy_tuple=None config=None max_out_tokens=1024 min_out_tokens=1 transposed_mode=False mpu=None ep_size=1 ep_group=None ep_mp_group=None moe_experts=[1] moe_type='standard' tp_proportion=(3, 3)
10.0.16.14: [2024-05-11 08:00:39,762] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1
10.0.16.14: [2024-05-11 08:00:39,764] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
10.0.16.14: [2024-05-11 08:00:39,764] [INFO] [comm.py:594:init_distributed] cdb=None
10.0.16.14: [2024-05-11 08:00:39,764] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
10.0.16.14: [2024-05-11 08:00:39,835] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5+fc9e1ee0, git-hash=fc9e1ee0, git-branch=HEAD
10.0.16.14: config_dict: {'mp_size': 4, 'dtype': torch.float32, 'tp_proportion': (3, 3)}
10.0.16.14: [2024-05-11 08:00:39,836] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead
10.0.16.14: ds_inference_config: replace_with_kernel_inject=False dtype=torch.float32 tensor_parallel=DeepSpeedTPConfig(enabled=True, tp_size=4, mpu=None, tp_group=None) enable_cuda_graph=False zero=DeepSpeedZeroConfig(stage=0, contiguous_gradients=True, reduce_scatter=True, reduce_bucket_size=500,000,000, allgather_partitions=True, allgather_bucket_size=500,000,000, overlap_comm=False, load_from_fp32_weights=True, elastic_checkpoint=False, offload_param=None, offload_optimizer=None, sub_group_size=1,000,000,000, cpu_offload_param=None, cpu_offload_use_pin_memory=None, cpu_offload=None, prefetch_bucket_size=50,000,000, param_persistence_threshold=100,000, model_persistence_threshold=sys.maxsize, max_live_parameters=1,000,000,000, max_reuse_distance=1,000,000,000, gather_16bit_weights_on_model_save=False, stage3_gather_fp16_weights_on_model_save=False, ignore_unused_parameters=True, legacy_stage1=False, round_robin_gradients=False, mics_shard_size=-1, mics_hierarchical_params_gather=False, memory_efficient_linear=True) triangular_masking=True moe=DeepSpeedMoEConfig(enabled=True, ep_size=1, moe_experts=[1], type='standard', ep_mp_group=None, ep_group=None) quant=QuantizationConfig(enabled=True, activation=ActivationQuantConfig(q_type='symmetric', q_groups=1, enabled=True, num_bits=8), weight=WeightQuantConfig(q_type='symmetric', q_groups=1, enabled=True, num_bits=8), qkv=QKVQuantConfig(enabled=True)) checkpoint=None base_dir=None set_empty_params=False save_mp_checkpoint_path=None checkpoint_config=InferenceCheckpointConfig(checkpoint_dir=None, save_mp_checkpoint_path=None, base_dir=None) return_tuple=True training_mp_size=1 replace_method='auto' injection_policy=None injection_policy_tuple=None config=None max_out_tokens=1024 min_out_tokens=1 transposed_mode=False mpu=None ep_size=1 ep_group=None ep_mp_group=None moe_experts=[1] moe_type='standard' tp_proportion=(3, 3)
10.0.16.14: [2024-05-11 08:00:39,836] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1
10.0.16.14: [2024-05-11 08:00:39,838] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
10.0.16.14: [2024-05-11 08:00:39,838] [INFO] [comm.py:594:init_distributed] cdb=None
10.0.16.13: [2024-05-11 16:00:39,990] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5+fc9e1ee0, git-hash=fc9e1ee0, git-branch=HEAD
10.0.16.13: config_dict: {'mp_size': 4, 'dtype': torch.float32, 'tp_proportion': (3, 3)}
10.0.16.13: [2024-05-11 16:00:39,991] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead
10.0.16.13: ds_inference_config: replace_with_kernel_inject=False dtype=torch.float32 tensor_parallel=DeepSpeedTPConfig(enabled=True, tp_size=4, mpu=None, tp_group=None) enable_cuda_graph=False zero=DeepSpeedZeroConfig(stage=0, contiguous_gradients=True, reduce_scatter=True, reduce_bucket_size=500,000,000, allgather_partitions=True, allgather_bucket_size=500,000,000, overlap_comm=False, load_from_fp32_weights=True, elastic_checkpoint=False, offload_param=None, offload_optimizer=None, sub_group_size=1,000,000,000, cpu_offload_param=None, cpu_offload_use_pin_memory=None, cpu_offload=None, prefetch_bucket_size=50,000,000, param_persistence_threshold=100,000, model_persistence_threshold=sys.maxsize, max_live_parameters=1,000,000,000, max_reuse_distance=1,000,000,000, gather_16bit_weights_on_model_save=False, stage3_gather_fp16_weights_on_model_save=False, ignore_unused_parameters=True, legacy_stage1=False, round_robin_gradients=False, mics_shard_size=-1, mics_hierarchical_params_gather=False, memory_efficient_linear=True) triangular_masking=True moe=DeepSpeedMoEConfig(enabled=True, ep_size=1, moe_experts=[1], type='standard', ep_mp_group=None, ep_group=None) quant=QuantizationConfig(enabled=True, activation=ActivationQuantConfig(q_type='symmetric', q_groups=1, enabled=True, num_bits=8), weight=WeightQuantConfig(q_type='symmetric', q_groups=1, enabled=True, num_bits=8), qkv=QKVQuantConfig(enabled=True)) checkpoint=None base_dir=None set_empty_params=False save_mp_checkpoint_path=None checkpoint_config=InferenceCheckpointConfig(checkpoint_dir=None, save_mp_checkpoint_path=None, base_dir=None) return_tuple=True training_mp_size=1 replace_method='auto' injection_policy=None injection_policy_tuple=None config=None max_out_tokens=1024 min_out_tokens=1 transposed_mode=False mpu=None ep_size=1 ep_group=None ep_mp_group=None moe_experts=[1] moe_type='standard' tp_proportion=(3, 3)
10.0.16.13: [2024-05-11 16:00:39,991] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1
10.0.16.13: [2024-05-11 16:00:39,994] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
10.0.16.13: [2024-05-11 16:00:39,994] [INFO] [comm.py:594:init_distributed] cdb=None
10.0.16.13: [2024-05-11 16:00:40,038] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5+fc9e1ee0, git-hash=fc9e1ee0, git-branch=HEAD
10.0.16.13: config_dict: {'mp_size': 4, 'dtype': torch.float32, 'tp_proportion': (3, 3)}
10.0.16.13: [2024-05-11 16:00:40,039] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead
10.0.16.13: ds_inference_config: replace_with_kernel_inject=False dtype=torch.float32 tensor_parallel=DeepSpeedTPConfig(enabled=True, tp_size=4, mpu=None, tp_group=None) enable_cuda_graph=False zero=DeepSpeedZeroConfig(stage=0, contiguous_gradients=True, reduce_scatter=True, reduce_bucket_size=500,000,000, allgather_partitions=True, allgather_bucket_size=500,000,000, overlap_comm=False, load_from_fp32_weights=True, elastic_checkpoint=False, offload_param=None, offload_optimizer=None, sub_group_size=1,000,000,000, cpu_offload_param=None, cpu_offload_use_pin_memory=None, cpu_offload=None, prefetch_bucket_size=50,000,000, param_persistence_threshold=100,000, model_persistence_threshold=sys.maxsize, max_live_parameters=1,000,000,000, max_reuse_distance=1,000,000,000, gather_16bit_weights_on_model_save=False, stage3_gather_fp16_weights_on_model_save=False, ignore_unused_parameters=True, legacy_stage1=False, round_robin_gradients=False, mics_shard_size=-1, mics_hierarchical_params_gather=False, memory_efficient_linear=True) triangular_masking=True moe=DeepSpeedMoEConfig(enabled=True, ep_size=1, moe_experts=[1], type='standard', ep_mp_group=None, ep_group=None) quant=QuantizationConfig(enabled=True, activation=ActivationQuantConfig(q_type='symmetric', q_groups=1, enabled=True, num_bits=8), weight=WeightQuantConfig(q_type='symmetric', q_groups=1, enabled=True, num_bits=8), qkv=QKVQuantConfig(enabled=True)) checkpoint=None base_dir=None set_empty_params=False save_mp_checkpoint_path=None checkpoint_config=InferenceCheckpointConfig(checkpoint_dir=None, save_mp_checkpoint_path=None, base_dir=None) return_tuple=True training_mp_size=1 replace_method='auto' injection_policy=None injection_policy_tuple=None config=None max_out_tokens=1024 min_out_tokens=1 transposed_mode=False mpu=None ep_size=1 ep_group=None ep_mp_group=None moe_experts=[1] moe_type='standard' tp_proportion=(3, 3)
10.0.16.13: [2024-05-11 16:00:40,040] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1
10.0.16.13: [2024-05-11 16:00:40,042] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
10.0.16.13: [2024-05-11 16:00:40,042] [INFO] [comm.py:594:init_distributed] cdb=None
10.0.16.14: tensor_parallel_group: <torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc285cb0cb0>
10.0.16.13: tensor_parallel_group: <torch.distributed.distributed_c10d.ProcessGroup object at 0x7f271960b1b0>
10.0.16.13: !deepspeed engine has no moe layers
10.0.16.14: tensor_parallel_group: <torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe625f92df0>
10.0.16.13: tensor_parallel_group: <torch.distributed.distributed_c10d.ProcessGroup object at 0x7fee8db24330>
10.0.16.13: !deepspeed engine has no moe layers
10.0.16.14: _replace_module: T5ForConditionalGeneration(
10.0.16.14:   (shared): Embedding(32128, 768)
10.0.16.14:   (encoder): T5Stack(
10.0.16.14:     (embed_tokens): Embedding(32128, 768)
10.0.16.14:     (block): ModuleList(
10.0.16.14:       (0): T5Block(
10.0.16.14:         (layer): ModuleList(
10.0.16.14:           (0): T5LayerSelfAttention(
10.0.16.14:             (SelfAttention): T5Attention(
10.0.16.14:               (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (relative_attention_bias): Embedding(32, 12)
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:           (1): T5LayerFF(
10.0.16.14:             (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:               (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:               (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:               (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:               (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:               (act): NewGELUActivation()
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:         )
10.0.16.14:       )
10.0.16.14:       (1-11): 11 x T5Block(
10.0.16.14:         (layer): ModuleList(
10.0.16.14:           (0): T5LayerSelfAttention(
10.0.16.14:             (SelfAttention): T5Attention(
10.0.16.14:               (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:           (1): T5LayerFF(
10.0.16.14:             (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:               (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:               (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:               (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:               (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:               (act): NewGELUActivation()
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:         )
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:     (final_layer_norm): T5LayerNorm()
10.0.16.14:     (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:   )
10.0.16.14:   (decoder): T5Stack(
10.0.16.14:     (embed_tokens): Embedding(32128, 768)
10.0.16.14:     (block): ModuleList(
10.0.16.14:       (0): T5Block(
10.0.16.14:         (layer): ModuleList(
10.0.16.14:           (0): T5LayerSelfAttention(
10.0.16.14:             (SelfAttention): T5Attention(
10.0.16.14:               (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (relative_attention_bias): Embedding(32, 12)
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:           (1): T5LayerCrossAttention(
10.0.16.14:             (EncDecAttention): T5Attention(
10.0.16.14:               (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:           (2): T5LayerFF(
10.0.16.14:             (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:               (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:               (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:               (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:               (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:               (act): NewGELUActivation()
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:         )
10.0.16.14:       )
10.0.16.14:       (1-11): 11 x T5Block(
10.0.16.14:         (layer): ModuleList(
10.0.16.14:           (0): T5LayerSelfAttention(
10.0.16.14:             (SelfAttention): T5Attention(
10.0.16.14:               (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:           (1): T5LayerCrossAttention(
10.0.16.14:             (EncDecAttention): T5Attention(
10.0.16.14:               (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:               (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:           (2): T5LayerFF(
10.0.16.14:             (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:               (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:               (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:               (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:               (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:               (act): NewGELUActivation()
10.0.16.14:             )
10.0.16.14:             (layer_norm): T5LayerNorm()
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           )
10.0.16.14:         )
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:     (final_layer_norm): T5LayerNorm()
10.0.16.14:     (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:   )
10.0.16.14:   (lm_head): Linear(in_features=768, out_features=32128, bias=False)
10.0.16.14: )
10.0.16.14: _replace_module: Embedding(32128, 768)
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.13: slice embedding
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.13: slice embedding
10.0.16.14: _replace_module: T5Stack(
10.0.16.14:   (embed_tokens): Embedding(32128, 768)
10.0.16.14:   (block): ModuleList(
10.0.16.14:     (0): T5Block(
10.0.16.14:       (layer): ModuleList(
10.0.16.14:         (0): T5LayerSelfAttention(
10.0.16.14:           (SelfAttention): T5Attention(
10.0.16.14:             (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (relative_attention_bias): Embedding(32, 12)
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:         (1): T5LayerFF(
10.0.16.14:           (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:             (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:             (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:             (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:             (act): NewGELUActivation()
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:     (1-11): 11 x T5Block(
10.0.16.14:       (layer): ModuleList(
10.0.16.14:         (0): T5LayerSelfAttention(
10.0.16.14:           (SelfAttention): T5Attention(
10.0.16.14:             (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:         (1): T5LayerFF(
10.0.16.14:           (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:             (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:             (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:             (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:             (act): NewGELUActivation()
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:       )
10.0.16.14:     )
10.0.16.13: replace without policy
10.0.16.14:   )
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14:   (final_layer_norm): T5LayerNorm()
10.0.16.14:   (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14: )
10.0.16.14: _replace_module: Embedding(32128, 768)
10.0.16.14: _replace_module: ModuleList(
10.0.16.14:   (0): T5Block(
10.0.16.14:     (layer): ModuleList(
10.0.16.14:       (0): T5LayerSelfAttention(
10.0.16.14:         (SelfAttention): T5Attention(
10.0.16.14:           (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (relative_attention_bias): Embedding(32, 12)
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:       (1): T5LayerFF(
10.0.16.14:         (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:           (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:           (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:           (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           (act): NewGELUActivation()
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:   )
10.0.16.14:   (1-11): 11 x T5Block(
10.0.16.14:     (layer): ModuleList(
10.0.16.14:       (0): T5LayerSelfAttention(
10.0.16.14:         (SelfAttention): T5Attention(
10.0.16.14:           (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:       (1): T5LayerFF(
10.0.16.14:         (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:           (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:           (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:           (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           (act): NewGELUActivation()
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:   )
10.0.16.14: )
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: slice embedding
10.0.16.14: slice embedding
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: replace without policyinner_dim: 768
10.0.16.14: 
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policyreplace without policy
10.0.16.14: 
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: 
10.0.16.13: replace without policy
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.13: replace without policy
10.0.16.14: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')replace without policy
10.0.16.14: 
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.14: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.13: slice embedding
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')replace without policy
10.0.16.14: 
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.13: slice embedding
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: _replace_module: T5LayerNorm()
10.0.16.14: _replace_module: Dropout(p=0.1, inplace=False)
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: _replace_module: T5Stack(
10.0.16.14:   (embed_tokens): Embedding(32128, 768)
10.0.16.14:   (block): ModuleList(
10.0.16.14:     (0): T5Block(
10.0.16.14:       (layer): ModuleList(
10.0.16.14:         (0): T5LayerSelfAttention(
10.0.16.14:           (SelfAttention): T5Attention(
10.0.16.14:             (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (relative_attention_bias): Embedding(32, 12)
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.13: replace without policy
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:         (1): T5LayerCrossAttention(
10.0.16.14:           (EncDecAttention): T5Attention(
10.0.16.14:             (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:         (2): T5LayerFF(
10.0.16.14:           (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:             (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14:             (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:             (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:             (act): NewGELUActivation()
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:     (1-11): 11 x T5Block(
10.0.16.14:       (layer): ModuleList(
10.0.16.14:         (0): T5LayerSelfAttention(
10.0.16.14:           (SelfAttention): T5Attention(
10.0.16.14:             (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:         (1): T5LayerCrossAttention(
10.0.16.14:           (EncDecAttention): T5Attention(
10.0.16.14:             (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:             (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:         (2): T5LayerFF(
10.0.16.14:           (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:             (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:             (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:             (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:             (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:             (act): NewGELUActivation()
10.0.16.14:           )
10.0.16.14:           (layer_norm): T5LayerNorm()
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:         )
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:   )
10.0.16.14:   (final_layer_norm): T5LayerNorm()
10.0.16.14:   (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14: )
10.0.16.14: slice embedding
10.0.16.14: _replace_module: Embedding(32128, 768)
10.0.16.14: _replace_module: ModuleList(
10.0.16.14:   (0): T5Block(
10.0.16.14:     (layer): ModuleList(
10.0.16.14:       (0): T5LayerSelfAttention(
10.0.16.14:         (SelfAttention): T5Attention(
10.0.16.14:           (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (relative_attention_bias): Embedding(32, 12)
10.0.16.14:         )
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:       (1): T5LayerCrossAttention(
10.0.16.14:         (EncDecAttention): T5Attention(
10.0.16.14:           (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:       (2): T5LayerFF(
10.0.16.14:         (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:           (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:           (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:           (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           (act): NewGELUActivation()
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:   )
10.0.16.14:   (1-11): 11 x T5Block(
10.0.16.14:     (layer): ModuleList(
10.0.16.14:       (0): T5LayerSelfAttention(
10.0.16.14:         (SelfAttention): T5Attention(
10.0.16.14:           (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:       (1): T5LayerCrossAttention(
10.0.16.14:         (EncDecAttention): T5Attention(
10.0.16.14:           (q): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (k): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (v): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:           (o): Linear(in_features=768, out_features=768, bias=False)
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:       (2): T5LayerFF(
10.0.16.14:         (DenseReluDense): T5DenseGatedActDense(
10.0.16.14:           (wi_0): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:           (wi_1): Linear(in_features=768, out_features=2048, bias=False)
10.0.16.14:           (wo): Linear(in_features=2048, out_features=768, bias=False)
10.0.16.13: replace without policy
10.0.16.14:           (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:           (act): NewGELUActivation()
10.0.16.14:         )
10.0.16.14:         (layer_norm): T5LayerNorm()
10.0.16.14:         (dropout): Dropout(p=0.1, inplace=False)
10.0.16.14:       )
10.0.16.14:     )
10.0.16.14:   )
10.0.16.14: )
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: slice embedding
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policyreplace without policy
10.0.16.14: 
10.0.16.13: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.13: replace without policy
10.0.16.14: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.13: replace without policy
10.0.16.13: all_reduce_linears: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.13: replace without policy
10.0.16.14: replace without policy
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: replace without policy
10.0.16.13: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.13: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'SelfAttention.o', 'DenseReluDense.wo')
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: replace without policy
10.0.16.14: all_reduce_linears: ('EncDecAttention.o', 'DenseReluDense.wo', 'SelfAttention.o')
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: SelfAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: n_heads: 12
10.0.16.14: inner_dim: 768
10.0.16.14: name: EncDecAttention.o in all_reduce_linears, weight shape: torch.Size([768, 768])
10.0.16.14: name: DenseReluDense.wo in all_reduce_linears, weight shape: torch.Size([768, 2048])
10.0.16.14: _replace_module: T5LayerNorm()
10.0.16.14: _replace_module: Dropout(p=0.1, inplace=False)
10.0.16.14: _replace_module: Linear(in_features=768, out_features=32128, bias=False)
10.0.16.14: rank: 0 end2end time is 2296.0944528932923 ms
10.0.16.14: rank: 0 model time is 2116.202059145327 ms
10.0.16.13: rank: 2 end2end time is 2295.9990589706986 ms
10.0.16.13: rank: 2 model time is 2096.292977227105 ms
10.0.16.13: [2024-05-11 16:01:51,355] [INFO] [launch.py:347:main] Process 2182826 exits successfully.
10.0.16.13: [2024-05-11 16:01:51,356] [INFO] [launch.py:347:main] Process 2182825 exits successfully.
10.0.16.14: [2024-05-11 08:01:51,550] [INFO] [launch.py:347:main] Process 42398 exits successfully.
10.0.16.14: [2024-05-11 08:01:51,550] [INFO] [launch.py:347:main] Process 42397 exits successfully.
